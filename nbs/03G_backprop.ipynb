{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop: The forward and backward passes\n",
    "L13-L14 2022p2\n",
    "See also [Simple Neural Net Backward Pass - Deriving the math of the backward pass for a simple neural net](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preliminaries: imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#mpl.rcParams['image.cmap'] = 'gray'. # default colormap for displaying images in Matplotlib.\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the MNIST data as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: \n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Basic architecture\n",
    "Lets start by defining a few variables: `n` is the number of training examples, `m` is the number of pixels, `c` is the number of possible values of digits: 50,000 training samples, 784 pixels and 10 possible digits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1   # number of values\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide (ahead of time) how many \"line segments\" to add up. \n",
    "The number in a layer is the *number of hidden nodes or activations*, `nh`.\n",
    "Lets arbitrarily decide `nh=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create lots of \"lines\", which we are then going to truncate at zero we do a matrix multiplication. \n",
    "Later we're going to have 50000x784 to multiply by a 784x10.\n",
    "But to simplify our starting point, lets give layer 2 just 1 output, so we can use MSE.\n",
    "[torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html) returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)\n",
    "b2 = torch.zeros(1)\n",
    "w1.shape, b1.shape, w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also lets use the smaller `x_valid` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear layer `lin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call it and returns a `[10000,50]` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 50]),\n",
       " tensor([[ -0.09,  11.87, -11.39,  ...,   5.48,   2.14,  15.30],\n",
       "         [  5.38,  10.21, -14.49,  ...,   0.88,   0.08,  20.23],\n",
       "         [  3.31,   0.12,   3.10,  ...,  16.89,  -6.05,  24.74],\n",
       "         ...,\n",
       "         [  4.01,  10.35, -11.25,  ...,   0.23,  -5.30,  18.28],\n",
       "         [ 10.62,  -4.27,  10.72,  ...,  -2.87,  -2.87,  18.23],\n",
       "         [  2.84,  -0.22,   1.43,  ...,  -3.91,   5.75,   2.12]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_valid, w1, b1)\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n",
       "        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n",
       "        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n",
       "        ...,\n",
       "        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n",
       "        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n",
       "        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(x): return x.clamp_min(0.)\n",
    "\n",
    "t = relu(t); t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lets define a basic Multi Layer Perceptron (MLP) from scratch.\n",
    "```python\n",
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compressed model\n",
    "def model(xb):\n",
    "    return lin(relu(lin(xb, w1, b1)), w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loss function: MSE - Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: `mse` is <u>not</u> a suitable loss function for multi-class classification; We'll use `mse` for now to keep things simple.\n",
    "</br>\n",
    "Layer two will be a matrix that goes from 50 hidden to one output (to simplify some calculations), because we are not going to use cross entropy yet but MSE.\n",
    "The one output will be a predictor what digit it is from 0 to 9.\n",
    "We compare those predictors (with little ^) to the actual labels. \n",
    "Let's say we predict 9 and the actual is 2, and we'll compare those together using MSE,  \n",
    "(a stupid way because it's saying that 9 is further away from being 2 than 2).\n",
    "9 is further away from 2 than it is from 4 in terms of how correct it is, which is not what we want, \n",
    "but to simplify the starting point, we have a single  output for the weight matrix and a single output for the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just substract, broadcasting creates a problem as it creates a huge matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-y_valid).shape   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of that trailing axis of `res` (,1), in order to use `mse`.\n",
    "We either use the single column of `res[:,0]` or we `res.squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000]), torch.Size([10000]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,0].shape, res.squeeze().shape # either use the single column or use squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res[:,0]-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use MSE we need the values of the labels `y` to be floats, but they are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4), tensor(4.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2], y_train[2].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the training and validation sets into floats as we're using MSE.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate our predictions for the training set, `x_train`, which is `[50000,1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]),\n",
       " tensor([[-30.97],\n",
       "         [-99.38],\n",
       "         [  8.72],\n",
       "         ...,\n",
       "         [-52.12],\n",
       "         [-46.25],\n",
       "         [ -4.35]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_train)\n",
    "preds.shape, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an `mse` function that does the subtraction of the passed arguments, squares it `.pow(2)` and takes the mean.\n",
    "And apply `mse` to the predictions `preds`, and the labels of the training set, `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4308.76)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n",
    "\n",
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gradients and backward pass\n",
    "\n",
    "See also [Simple Neural Net Backward Pass - Deriving the math of the backward pass for a simple neural net.](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing gradients of a linear layer\n",
    "`lin_grad` computes the gradient of a linear layer.\n",
    "Per the chain rule, we need: the input `inp`, output `out`, weights `w`, and the biases `b` of the layer.\n",
    "\n",
    "We will store the gradients of our input in `inp.g`, \n",
    "which is `out.g @ w.t()` the gradients of `out` with respect to the input times the weights (transposed).\n",
    "A matrix multiplier is a whole bunch of linear functions, so each one slope is just its weight.\n",
    "But we have to multiply it by the gradient of the outputs `out.g` because of the chain rule.\n",
    "\n",
    "The gradient of the outputs with respect to the weights, `w.g`, is the input times the output summed up.\n",
    "Every input weights has to be multiplied by the outputs, that's why we have to do an `unsqueeze(-1)`. \n",
    "\n",
    "The  derivatives of the bias, `b.g`, is the gradients of the output added together because the bias is just a constant value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):  #the gradient of a linear layer\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t() #he gradients of the input, inp.g, is the gradients of out with respect to the input times the weights.\n",
    "    #import pdb; pdb.set_trace()\n",
    "    i, o = inp.unsqueeze(-1) , out.g.unsqueeze(1)\n",
    "    w.g = (i * o).sum(0) #The gradient of the outputs with respect to the weights, w.g, is the input times the output summed up\n",
    "    b.g = out.g.sum(0) #The derivatives of the bias, b.g, is the gradients of the output added together because the bias is just a constant value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Forward and Backward passes\n",
    "The **forward** pass is where we calculate the `loss`, which is `diff` \n",
    "(the output `out` of the ANN minus our target `targ`), squared and then take the mean. \n",
    "`out` is the output of the 2nd linear layer `l2`. \n",
    "The input to `l2` is the ReLU, and the ReLU's input is the first layer, `l1`. \n",
    "We take the input,`inp` put it through a linear layer `l1`, through a ReLU, \n",
    "through a linear layer `l2` and calculate the MSE.\n",
    "\n",
    "In the **backward** pass, we store the gradients of each layer (e.g., loss with respect to inputs), in the layer itself.\n",
    "We define a new attribute `g`, e.g., `out.g`, to contain the gradients.\n",
    "In `out.g = 2.*diff[:,None] / inp.shape[0]` the derivative is two times the difference because we've got difference squared.  \n",
    "We took the `.mean()` when computing the loss, so we have to do the same thing here, i.e., divide by the `inp.shape[0]`.\n",
    "Now we need to multiply by the gradients of the previous layer, `l2`.\n",
    "We use `lin_grad` to compute the gradients of a linear layer. \n",
    "Per the chain rule, we need: the weights `w2`, the biases `b2`, and also the input `l2` and the output `out` from the linear layer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]  # the gradients saved\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save copies of all the gradients for `w1,w2,b1,b2,x_train`, in a list `grads` (for testing against the Pytorch equivalents later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = map(get_grad, chks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save all the Pytorch computed equivalent gradients, `w12,w22,b12,b22,xt2`, in a list `ptgrads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = map(mkgrad, chks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just run it all through PyTorch and check that their derivatives `ptgrads` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We \"cheat a little bit\" and use PyTorch autograd to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the calculated derivatives `grads` by comparing them with `ptgrads`: they are the same derivatives calculated by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): test_close(a.grad, b, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Refactor using Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refactor and simplify by using classes and invoking them as functions.\n",
    "Lets illustrate by a class just to print hello.\n",
    "We create an instance of that class and then call it as if it was a function.\n",
    "In Python by defining `__call__` we can treat a class as if it's a function without any method at all. \n",
    "We can still do it the method way, but why do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes: `ReLU` and `Lin`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a `ReLU` class and add `__call__` so we can treat it as a function. \n",
    "Note that the `backward` pass has to know about the intermediate calculations\n",
    "because of the chain rule, and because of how the derivatives are calculated.\n",
    "We need to store each of the layer intermediate calculations.\n",
    "The `ReLU` class stores its output `out` and its input `inp`, so when we call the `backward` method, we \n",
    "use them to calculate the inputs gradient, `self.inp.g`, by the chain rule as the product of 2 derivatives,\n",
    "`(self.inp>0).float() * self.out.g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a linear layer class `Lin` we need additional state to be passed: weights and  biases. (ReLU doesn't). \n",
    "We indicate its weights `w` and biases `b`, and store them in `__init__`\n",
    "When we `__call__` it on the forward pass we store the input `inp`, then \n",
    "compute the output by calling `lin`, store it in `self.out`, and `return` it. <br>\n",
    "For the `backward` pass, the input gradients we calculate as before. \n",
    "`.t()` is the same as `T` is as a property: the transpose.  \n",
    "We calculate the gradients of `inp, w, b` and store them in the corresponding `.g` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below to compute the derivation of the gradients in `backward()`:\n",
    "```python   \n",
    "#Below 4 lines are all gradients of the loss with respect to\n",
    "dJ_dZ = self.out.g  #  the output     \n",
    "self.w.g = dJ_dW = self.inp.t() @ dJ_dZ #  w_j     \n",
    "self.b.g = dJ_db = dJ_dZ.sum(0)   # bias b\n",
    "self.inp.g = dJ_dX = dJ_dZ @ self.w.t() # to X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()  # See Gradient of loss with respect to X   \n",
    "        self.w.g = self.inp.t() @ self.out.g # See Gradient of loss with respect to w_j\n",
    "        self.b.g = self.out.g.sum(0) # See Gradient of loss with respect to the bias b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward function of the `Mse` class below computes an estimate of how the loss function changes as the input activations change.\n",
    "We calculate `mse()` and store it in `.out`. \n",
    "MSE needs input and target, so we store them in `.inp, .out`. \n",
    "In the backward pass we can calculate its gradient of the input as being two times the difference. \n",
    "For the backward we compute it as:\n",
    "```python\n",
    "N = self.targ.shape[0] ;  A = self.inp.squeeze() ; Y = self.targ\n",
    "self.inp.g = dJ_dA = (2./N) * (A - Y).unsqueeze(-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model\n",
    "The model is easier to define as a list of layers, `[Lin(w1,b1), Relu(), Lin(w2,b2)]`. \n",
    "We store in `self.loss` an instance of `Mse()`.\n",
    "NB: These are not calls, just instances of the classes (`Lin, Relu, Mse`) being stored, \n",
    "so when we call the model we pass it inputs and targets. \n",
    "In `__call__` we go through each layer, set `x` equal to the result of calling that layer, and then pass that to the `loss`. \n",
    "<br>\n",
    "**NB:** We don't have two separate functions, the loss function being applied to a separate neural net.\n",
    "Rather we integrated the loss function into the model, i.e., the `loss` is calculated inside the model.\n",
    "That is different, neither better nor worse than having it separately.\n",
    "HuggingFace does it this way, it puts the `loss` inside the `forward`.\n",
    "Fastai and other libraries does it separately, i.e., loss is a whole separate function, \n",
    "and the model only returns the result of putting it through the layers.\n",
    "Here, as in HF, the loss function is inside the model.\n",
    "<br>\n",
    "For backward, `self.loss` is the `Mse()` object. \n",
    "So that's going to call the `Mse` class `.backward()`, that will store the inputs, the targets, the outputs, \n",
    "so it can calculate the `backward()`.\n",
    "Then we go through each layer in reverse, the **back propagation**, `backwards` `reversed`, calling `l.backward()` on each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: if we just return the `loss` above in the `__call__`, how do you get predictions?  \n",
    "A: HuggingFace models return not just the `loss`, but a dictionary, \n",
    "i.e., `dict(loss=..., preds=...)`, something like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the `Model`, calculate the `loss`, and call `backward`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "\n",
    "loss = model(x_train, y_train)\n",
    "\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can check that each of the gradients that we stored earlier are equal to each of our new gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()\n",
    "\n",
    "Repeated code,  e.g., `self.inp=inp`, etc., is a sign that we can refactor things.  \n",
    "Lets define a new class `Module()` to do the repeated code. \n",
    "`Module` will store the inputs, call `self.forward` to create the `self.out`, and then return it. \n",
    "There is a do nothing `forward` in this class, as the purpose of `Module` is to be inherited. \n",
    "When we call `backward`, it will call `self.bwd` passing in 2 arguments:\n",
    "(1) `self.out` as all `backwards()` wanted to get `self.out` because of the chain rule,\n",
    "and (2) the arguments `args` that we stored earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: A `*` in a *signature* before the parameter name (e.g., `def f1(*args)` allows the function to accept any number of positional arguments, regardless of number, and put them into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are often opportunities to manually speed-up by defining custom Pytorch autograd functions.\n",
    "For example, in `Mse` a calculation  `inp.squeeze() - targ` is being done twice.\n",
    "At the cost of some memory, we could instead store that calculation as, e.g., `self.diff`, and remove a redundant calculation.\n",
    "We often can do a compromise between memory use and the computational speedup of not having to recalculate it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call it in the same way, create the model, passing in all of those layers. \n",
    "The model  hasn't changed at this point. \n",
    "The definition was up here, we just pass in the weights for the layers,\n",
    "calculate the loss, call backward, and it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "\n",
    "loss = model(x_train, y_train)\n",
    "\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "Since we reimplemented it, we can now use PyTorch's version, `nn.Module`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a `Linear` layer we inherit from `nn.Module`.\n",
    "Here, rather than passing in the already randomized weights, we generate the random weights and the zeroed biases at `__init__`.\n",
    "We define `forward` but we don't need to define `backward`, as PyTorch \n",
    "knows all the derivatives and the chain rule, it will do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a `Model` class that uses `nn.Module`, it's the same as before,\n",
    "but now we use PyTorch's `F.mse_loss()`.<br>\n",
    "NB: We need the extra axis in `targ[:,None]` as we saw the problem if we don't have it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model, call backward.\n",
    "We stored our gradients in `.g`, PyTorch stores them in `.grad`.\n",
    "The same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n",
       "          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n",
       "        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n",
       "        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "To summarize, we've created from scratch:\n",
    "* a matrix multiplication\n",
    "* linear layers\n",
    "* a complete backprop system of modules \n",
    "\n",
    "We can now calculate both the forward pass and the  backward pass for linear layers and values so\n",
    "we can create a multilayer perceptron, and we can train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
