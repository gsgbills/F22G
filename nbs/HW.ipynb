{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb2525-0605-4974-b5e1-f64148fdec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Homeworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66fb13c-7890-4eea-b5d6-cf05e116d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd6dc92-ee28-47e3-b8d3-bf87cc5756f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=140, threshold=1000)\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False, threshold=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92025313-3e10-4983-a939-2ed17c599947",
   "metadata": {},
   "source": [
    "## 1. HW: Reimplement log_softmax(), nll_loss() and cross_entropy() and compare them to PyTorch's values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2bec73-7567-45cb-a707-81b8c531f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_softmax(x):\n",
    "    # Numerical stability: subtract the max\n",
    "    x_max = max(x)\n",
    "    shifted_x = [xi - x_max for xi in x]\n",
    "\n",
    "    # Compute log-sum-exp\n",
    "    sum_exp = sum(math.exp(xi) for xi in shifted_x)\n",
    "    logsumexp = math.log(sum_exp)\n",
    "\n",
    "    # Compute log_softmax\n",
    "    return [xi - logsumexp for xi in shifted_x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83036869-bbba-4700-a536-71d120eab2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ecf2dd0-4eb1-451d-83fe-6fb3033ee1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Softmax: ['-0.42', '-1.42', '-2.32']\n",
      "F.Log-Softmax: tensor([-0.42, -1.42, -2.32])\n"
     ]
    }
   ],
   "source": [
    "logits = [2.0, 1.0, 0.1]\n",
    "result = log_softmax(logits)\n",
    "print(\"Log-Softmax:\",[f\"{v:.2f}\" for v in result])\n",
    "result = F.log_softmax(tensor(logits),dim=-1)\n",
    "print(\"F.Log-Softmax:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838e102-9075-44e5-b6a9-4179a1eabf15",
   "metadata": {},
   "source": [
    "## nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72ee5047-e9a3-4234-a384-264a80fdd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(log_probs, targets, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Pure Python equivalent of PyTorch's torch.nn.functional.nll_loss.\n",
    "\n",
    "    Args:\n",
    "        log_probs (list of list of float): Log-probabilities (from log_softmax), shape [batch_size][num_classes]\n",
    "        targets (list of int): Indices of target classes for each sample, length == batch_size\n",
    "        reduction (str): One of 'none', 'mean', or 'sum'\n",
    "\n",
    "    Returns:\n",
    "        float or list of floats: Loss value(s)\n",
    "    \"\"\"\n",
    "    # --- Type & shape checks ---\n",
    "    assert isinstance(log_probs, list), \"log_probs must be a list\"\n",
    "    assert all(isinstance(row, list) for row in log_probs), \"Each row in log_probs must be a list\"\n",
    "    assert isinstance(targets, list), \"targets must be a list\"\n",
    "    assert all(isinstance(t, int) for t in targets), \"Each target must be an integer\"\n",
    "    assert len(log_probs) == len(targets), \"Number of samples and targets must match\"\n",
    "    assert reduction in ('none', 'mean', 'sum'), \"reduction must be 'none', 'mean', or 'sum'\"\n",
    "\n",
    "    # --- Compute losses ---\n",
    "    losses = []\n",
    "    for i, (log_row, target_idx) in enumerate(zip(log_probs, targets)):\n",
    "        assert 0 <= target_idx < len(log_row), f\"Invalid class index {target_idx} at row {i}\"\n",
    "        loss = -log_row[target_idx]\n",
    "        losses.append(loss)\n",
    "\n",
    "    # --- Apply reduction ---\n",
    "    if reduction == 'none':\n",
    "        return losses\n",
    "    elif reduction == 'sum':\n",
    "        return sum(losses)\n",
    "    elif reduction == 'mean':\n",
    "        return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba068616-0dc7-419a-b318-5d5461c226c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL Loss: 0.1\n"
     ]
    }
   ],
   "source": [
    "log_probs = [\n",
    "    [-0.1, -2.3, -2.3],  # sample 0\n",
    "    [-2.3, -0.1, -2.3],  # sample 1\n",
    "]\n",
    "targets = [0, 1]\n",
    "\n",
    "loss = nll_loss(log_probs, targets, reduction='mean')\n",
    "print(\"NLL Loss:\", loss)  # Output: 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c85a579-179d-4e89-8a3c-c3990c3145d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000149011612"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floss = F.nll_loss(tensor(log_probs), tensor(targets), reduction=\"mean\")\n",
    "floss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424845b-8a28-43f4-9aec-ed919bd5dafa",
   "metadata": {},
   "source": [
    "## cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75312803-6e58-4c15-b638-6c0662516060",
   "metadata": {},
   "source": [
    "## HW: TODO: recreate matrix multiply, forward and backward passes, something that steps through layers. Recreate the idea of .forward and .backward.\n",
    "Make sure to fully understand what's going on. First pick a smaller part of that, the most interesting part, or just go through and look really closely at these notebooks. Restart the notebook kernel and clear output, try to think what are the shapes and values of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412478b5-1d46-4138-ab12-1bf7c5861e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo HW Experiment with these to see what everything is taking in\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13405dd-5a60-4ab7-8f8b-95d6b39508bc",
   "metadata": {},
   "source": [
    "TODO: understand how we can pass a BatchSampler to sampler and what's it doing. For this, lets go back to our non-multi-processing pure Python code to see how that would work. It's a nifty trick to grab multiple things at once and it can save time, make code faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc699b-7f55-4e10-bd11-d3090f5459ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c501fd8-3e4b-4cb6-bb38-3f9aec0be699",
   "metadata": {},
   "source": [
    "TODO: inside collate_dict(ds) import pdb, pdb.set_trace(), put breakpoints, step through, see exactly what's happening. Also more importantly, set_trace inside the innermost _f function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eeebd7-4c99-4496-85ac-7546682bd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_dict(ds): \n",
    "    '''take a dict and collate it into a tuple.'''\n",
    "    get = itemgetter(*ds.features)\n",
    "    def _f(b): \n",
    "        #import pdb; pdb.set_trace()\n",
    "        return get(default_collate(b))\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b951ad-a3ac-43fa-ba5d-c52adcb05101",
   "metadata": {},
   "source": [
    "### Browsing source code and debugging\n",
    "\n",
    "    Jump to tag/symbol by with (with completions)\n",
    "    Jump to current tag\n",
    "    Jump to library tags\n",
    "    Go back\n",
    "    Search\n",
    "    Outlining / folding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4441e15-d67d-46ca-8af2-8e655052c27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
